{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Equity Classification Using Financial Ratios\n",
        "\n",
        "## Complete Machine Learning Pipeline\n",
        "\n",
        "This notebook contains the complete analysis pipeline:\n",
        "1. Data Loading & Validation\n",
        "2. Exploratory Data Analysis (EDA)\n",
        "3. Data Preprocessing & Missing Value Imputation\n",
        "4. Financial Ratio Computation\n",
        "5. Correlation Analysis & Feature Selection\n",
        "6. Model Training & Evaluation\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from scipy import stats\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Model Selection\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, \n",
        "                             accuracy_score, precision_score, recall_score, \n",
        "                             f1_score, roc_auc_score, roc_curve)\n",
        "\n",
        "# Settings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\" All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Data Loading & Validation\n",
        "\n",
        "Load your existing financial statement data.\n",
        "\n",
        "### Expected Data Format:\n",
        "\n",
        "Your CSV/Excel file should contain the following columns:\n",
        "\n",
        "**Identification Columns:**\n",
        "- `Company`: Company name or ticker\n",
        "- `Sector`: Industry sector (optional)\n",
        "- `Year`: Fiscal year\n",
        "- `Quarter`: Fiscal quarter (optional)\n",
        "\n",
        "**Income Statement:**\n",
        "- `Revenue`: Total revenue/sales\n",
        "- `COGS`: Cost of goods sold\n",
        "- `Gross_Profit`: Revenue - COGS\n",
        "- `Operating_Expenses`: Operating expenses\n",
        "- `EBIT`: Earnings before interest and tax\n",
        "- `Interest_Expense`: Interest payments\n",
        "- `EBT`: Earnings before tax\n",
        "- `Tax`: Tax expense\n",
        "- `Net_Income`: Net profit\n",
        "\n",
        "**Balance Sheet:**\n",
        "- `Current_Assets`: Current assets\n",
        "- `Cash`: Cash and equivalents\n",
        "- `Accounts_Receivable`: Receivables\n",
        "- `Inventory`: Inventory\n",
        "- `Fixed_Assets`: Non-current assets\n",
        "- `Total_Assets`: Total assets\n",
        "- `Current_Liabilities`: Current liabilities\n",
        "- `Accounts_Payable`: Payables\n",
        "- `Short_Term_Debt`: Short-term debt\n",
        "- `Long_Term_Debt`: Long-term debt\n",
        "- `Total_Liabilities`: Total liabilities\n",
        "- `Shareholders_Equity`: Equity\n",
        "\n",
        "**Cash Flow Statement:**\n",
        "- `Operating_Cash_Flow`: Cash from operations\n",
        "- `CapEx`: Capital expenditure\n",
        "- `Free_Cash_Flow`: OCF - CapEx\n",
        "\n",
        "**Market Data:**\n",
        "- `Shares_Outstanding`: Number of shares\n",
        "- `Stock_Price`: Share price\n",
        "- `Market_Cap`: Market capitalization\n",
        "- `EPS`: Earnings per share\n",
        "- `Dividend_Per_Share`: Dividend per share (optional)\n",
        "\n",
        "**Target Variable:**\n",
        "- `Investment_Quality`: 'Good' or 'Bad' (your classification label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# LOAD YOUR DATA HERE\n",
        "# ========================================\n",
        "\n",
        "# Option 1: Load from CSV\n",
        "file_path = 'your_financial_data.csv'  # â† Change this to your file path\n",
        "df_raw = pd.read_csv(file_path)\n",
        "\n",
        "# Option 2: Load from Excel\n",
        "# file_path = 'your_financial_data.xlsx'\n",
        "# df_raw = pd.read_excel(file_path, sheet_name='Sheet1')\n",
        "\n",
        "# Option 3: Load from multiple files (if you have separate files for each company/year)\n",
        "# import glob\n",
        "# files = glob.glob('financial_data/*.csv')\n",
        "# df_raw = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
        "\n",
        "print(f\" Data loaded successfully!\")\n",
        "print(f\"  Shape: {df_raw.shape}\")\n",
        "print(f\"  Companies: {df_raw['Company'].nunique() if 'Company' in df_raw.columns else 'N/A'}\")\n",
        "print(f\"  Years: {df_raw['Year'].min() if 'Year' in df_raw.columns else 'N/A'} - {df_raw['Year'].max() if 'Year' in df_raw.columns else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Data Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for required columns\n",
        "required_columns = [\n",
        "    'Company', 'Year', 'Revenue', 'Net_Income', 'Total_Assets', \n",
        "    'Total_Liabilities', 'Shareholders_Equity', 'Current_Assets', \n",
        "    'Current_Liabilities', 'Investment_Quality'\n",
        "]\n",
        "\n",
        "missing_cols = [col for col in required_columns if col not in df_raw.columns]\n",
        "\n",
        "if missing_cols:\n",
        "    print(\"WARNING: Missing required columns:\")\n",
        "    for col in missing_cols:\n",
        "        print(f\"   - {col}\")\n",
        "    print(\"\\n   Please ensure your data contains these columns.\")\n",
        "else:\n",
        "    print(\" All required columns present\")\n",
        "\n",
        "# Display available columns\n",
        "print(f\"\\nAvailable columns ({len(df_raw.columns)}):\")\n",
        "for i, col in enumerate(df_raw.columns, 1):\n",
        "    print(f\"{i:2d}. {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "print(\"First 5 rows of data:\")\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display last few rows\n",
        "print(\"Last 5 rows of data:\")\n",
        "df_raw.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data types and basic info\n",
        "print(\"Dataset Information:\")\n",
        "df_raw.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Data Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"Dataset Summary:\")\n",
        "print(f\"Total Records: {len(df_raw):,}\")\n",
        "print(f\"Unique Companies: {df_raw['Company'].nunique() if 'Company' in df_raw.columns else 'N/A'}\")\n",
        "print(f\"Time Period: {df_raw['Year'].min() if 'Year' in df_raw.columns else 'N/A'} - {df_raw['Year'].max() if 'Year' in df_raw.columns else 'N/A'}\")\n",
        "if 'Sector' in df_raw.columns:\n",
        "    print(f\"Sectors: {df_raw['Sector'].nunique()}\")\n",
        "print(f\"Columns: {len(df_raw.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Missing Values Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check missing values\n",
        "missing = df_raw.isnull().sum()\n",
        "missing_pct = (missing / len(df_raw)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing.index,\n",
        "    'Missing_Count': missing.values,\n",
        "    'Percentage': missing_pct.values\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(\"Missing Values Summary:\")\n",
        "    print(missing_df.to_string(index=False))\n",
        "    \n",
        "    # Visualize missing values\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.barh(missing_df['Column'], missing_df['Missing_Count'])\n",
        "    plt.xlabel('Number of Missing Values')\n",
        "    plt.title('Missing Values by Column')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\" No missing values found in the dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Target Variable Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if Investment_Quality column exists\n",
        "if 'Investment_Quality' in df_raw.columns:\n",
        "    # Target variable distribution\n",
        "    print(\"Investment Quality Distribution:\")\n",
        "    print(df_raw['Investment_Quality'].value_counts())\n",
        "    print(\"\\nPercentages:\")\n",
        "    print(df_raw['Investment_Quality'].value_counts(normalize=True) * 100)\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Count plot\n",
        "    df_raw['Investment_Quality'].value_counts().plot(kind='bar', ax=axes[0], color=['#51cf66', '#ff6b6b'])\n",
        "    axes[0].set_title('Investment Quality Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Investment Quality', fontsize=12)\n",
        "    axes[0].set_ylabel('Count', fontsize=12)\n",
        "    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Pie chart\n",
        "    df_raw['Investment_Quality'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
        "                                                      colors=['#51cf66', '#ff6b6b'])\n",
        "    axes[1].set_title('Investment Quality Distribution (%)', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"  WARNING: 'Investment_Quality' column not found in dataset\")\n",
        "    print(\"   Please ensure your data has a target variable for classification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Descriptive Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select key financial metrics that exist in the dataset\n",
        "potential_metrics = ['Revenue', 'Net_Income', 'Total_Assets', 'Total_Liabilities', \n",
        "                     'Shareholders_Equity', 'Operating_Cash_Flow', 'EPS', 'Stock_Price',\n",
        "                     'EBIT', 'Gross_Profit', 'Current_Assets', 'Current_Liabilities']\n",
        "\n",
        "key_metrics = [col for col in potential_metrics if col in df_raw.columns]\n",
        "\n",
        "if key_metrics:\n",
        "    print(\"Key Financial Metrics - Summary Statistics:\")\n",
        "    display(df_raw[key_metrics].describe().round(2))\n",
        "else:\n",
        "    print(\"Key financial metrics not found. Showing all numerical columns:\")\n",
        "    display(df_raw.describe().round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution plots for key metrics\n",
        "metrics_to_plot = [col for col in ['Revenue', 'Net_Income', 'Total_Assets', 'EPS', 'Operating_Cash_Flow', 'Stock_Price'] \n",
        "                   if col in df_raw.columns]\n",
        "\n",
        "if metrics_to_plot:\n",
        "    n_plots = len(metrics_to_plot)\n",
        "    n_cols = 3\n",
        "    n_rows = (n_plots + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "    axes = axes.ravel() if n_plots > 1 else [axes]\n",
        "    \n",
        "    for idx, metric in enumerate(metrics_to_plot):\n",
        "        data = df_raw[metric].dropna()\n",
        "        axes[idx].hist(data, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "        axes[idx].set_title(f'Distribution of {metric}', fontweight='bold')\n",
        "        axes[idx].set_xlabel(metric)\n",
        "        axes[idx].set_ylabel('Frequency')\n",
        "        axes[idx].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Add statistics\n",
        "        skew = data.skew()\n",
        "        kurt = data.kurtosis()\n",
        "        axes[idx].text(0.95, 0.95, f'Skew: {skew:.2f}\\nKurt: {kurt:.2f}',\n",
        "                      transform=axes[idx].transAxes,\n",
        "                      verticalalignment='top',\n",
        "                      horizontalalignment='right',\n",
        "                      bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for idx in range(n_plots, len(axes)):\n",
        "        axes[idx].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No standard financial metrics found for distribution analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6 Sector Analysis (if available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sector analysis if Sector column exists\n",
        "if 'Sector' in df_raw.columns:\n",
        "    print(\"Companies by Sector:\")\n",
        "    print(df_raw.groupby('Sector')['Company'].nunique().sort_values(ascending=False))\n",
        "    \n",
        "    if 'Investment_Quality' in df_raw.columns:\n",
        "        print(\"\\nInvestment Quality by Sector (%):\")\n",
        "        sector_quality = pd.crosstab(df_raw['Sector'], df_raw['Investment_Quality'], normalize='index') * 100\n",
        "        print(sector_quality.round(1))\n",
        "        \n",
        "        # Visualize\n",
        "        sector_quality.plot(kind='bar', figsize=(12, 6), stacked=False)\n",
        "        plt.title('Investment Quality by Sector', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Sector', fontsize=12)\n",
        "        plt.ylabel('Percentage', fontsize=12)\n",
        "        plt.legend(title='Investment Quality')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\" No 'Sector' column found. Skipping sector analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.7 Correlation Analysis (Raw Metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select numerical columns for correlation\n",
        "potential_corr_vars = ['Revenue', 'Gross_Profit', 'EBIT', 'Net_Income', \n",
        "                       'Total_Assets', 'Total_Liabilities', 'Shareholders_Equity',\n",
        "                       'Operating_Cash_Flow', 'Free_Cash_Flow', 'EPS']\n",
        "\n",
        "correlation_vars = [col for col in potential_corr_vars if col in df_raw.columns]\n",
        "\n",
        "if len(correlation_vars) >= 3:\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = df_raw[correlation_vars].corr()\n",
        "    \n",
        "    # Visualize correlation matrix\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title('Correlation Matrix of Financial Metrics', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Find highly correlated pairs\n",
        "    high_corr = []\n",
        "    for i in range(len(corr_matrix)):\n",
        "        for j in range(i+1, len(corr_matrix)):\n",
        "            if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
        "                high_corr.append({\n",
        "                    'Variable 1': corr_matrix.index[i],\n",
        "                    'Variable 2': corr_matrix.columns[j],\n",
        "                    'Correlation': corr_matrix.iloc[i, j]\n",
        "                })\n",
        "    \n",
        "    if high_corr:\n",
        "        high_corr_df = pd.DataFrame(high_corr).sort_values('Correlation', key=abs, ascending=False)\n",
        "        print(\"\\nHighly Correlated Pairs (|r| > 0.8):\")\n",
        "        print(high_corr_df.head(10).to_string(index=False))\n",
        "else:\n",
        "    print(\"Not enough numerical columns for correlation analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Data Preprocessing & Missing Value Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for processing\n",
        "df = df_raw.copy()\n",
        "\n",
        "# Check missing values before imputation\n",
        "missing_before = df.isnull().sum().sum()\n",
        "print(f\"Missing values before imputation: {missing_before:,}\")\n",
        "\n",
        "if missing_before > 0:\n",
        "    # Median imputation for numerical columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    # Exclude identifier columns from imputation\n",
        "    exclude_cols = ['Year', 'Quarter'] if 'Quarter' in numeric_cols else ['Year']\n",
        "    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "    \n",
        "    if numeric_cols:\n",
        "        print(f\"\\nImputing {len(numeric_cols)} numerical columns...\")\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
        "    \n",
        "    missing_after = df.isnull().sum().sum()\n",
        "    print(f\"Missing values after imputation: {missing_after:,}\")\n",
        "    print(f\" Imputed {missing_before - missing_after:,} missing values using median strategy\")\n",
        "else:\n",
        "    print(\"No missing values to impute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: Financial Ratio Computation\n",
        "\n",
        "We'll compute financial ratios across multiple categories:\n",
        "1. Profitability Ratios\n",
        "2. Liquidity Ratios\n",
        "3. Leverage/Solvency Ratios\n",
        "4. Efficiency/Activity Ratios\n",
        "5. Valuation Ratios\n",
        "6. Cash Flow Ratios\n",
        "7. Growth Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_divide(numerator, denominator, default=0):\n",
        "    \"\"\"Safely divide two series, handling division by zero\"\"\"\n",
        "    result = numerator / denominator\n",
        "    result = result.replace([np.inf, -np.inf], np.nan)\n",
        "    return result.fillna(default)\n",
        "\n",
        "print(\"Computing Financial Ratios...\\n\")\n",
        "\n",
        "ratios = {}\n",
        "computed_count = 0\n",
        "\n",
        "# ====================\n",
        "# 1. PROFITABILITY RATIOS\n",
        "# ====================\n",
        "print(\"1. Profitability Ratios\")\n",
        "profitability_count = 0\n",
        "\n",
        "if 'Gross_Profit' in df.columns and 'Revenue' in df.columns:\n",
        "    ratios['Gross_Profit_Margin'] = safe_divide(df['Gross_Profit'], df['Revenue']) * 100\n",
        "    profitability_count += 1\n",
        "\n",
        "if 'EBIT' in df.columns and 'Revenue' in df.columns:\n",
        "    ratios['Operating_Profit_Margin'] = safe_divide(df['EBIT'], df['Revenue']) * 100\n",
        "    profitability_count += 1\n",
        "\n",
        "if 'Net_Income' in df.columns and 'Revenue' in df.columns:\n",
        "    ratios['Net_Profit_Margin'] = safe_divide(df['Net_Income'], df['Revenue']) * 100\n",
        "    profitability_count += 1\n",
        "\n",
        "if 'Net_Income' in df.columns and 'Total_Assets' in df.columns:\n",
        "    ratios['ROA'] = safe_divide(df['Net_Income'], df['Total_Assets']) * 100\n",
        "    profitability_count += 1\n",
        "\n",
        "if 'Net_Income' in df.columns and 'Shareholders_Equity' in df.columns:\n",
        "    ratios['ROE'] = safe_divide(df['Net_Income'], df['Shareholders_Equity']) * 100\n",
        "    profitability_count += 1\n",
        "\n",
        "if 'Operating_Cash_Flow' in df.columns and 'Revenue' in df.columns:\n",
        "    ratios['OCF_Margin'] = safe_divide(df['Operating_Cash_Flow'], df['Revenue']) * 100\n",
        "    profitability_count += 1\n",
        "\n",
        "print(f\"    Computed {profitability_count} profitability ratios\")\n",
        "computed_count += profitability_count\n",
        "\n",
        "# ====================\n",
        "# 2. LIQUIDITY RATIOS\n",
        "# ====================\n",
        "print(\"2. Liquidity Ratios\")\n",
        "liquidity_count = 0\n",
        "\n",
        "if 'Current_Assets' in df.columns and 'Current_Liabilities' in df.columns:\n",
        "    ratios['Current_Ratio'] = safe_divide(df['Current_Assets'], df['Current_Liabilities'])\n",
        "    liquidity_count += 1\n",
        "\n",
        "if all(col in df.columns for col in ['Current_Assets', 'Inventory', 'Current_Liabilities']):\n",
        "    ratios['Quick_Ratio'] = safe_divide(df['Current_Assets'] - df['Inventory'], df['Current_Liabilities'])\n",
        "    liquidity_count += 1\n",
        "\n",
        "if 'Cash' in df.columns and 'Current_Liabilities' in df.columns:\n",
        "    ratios['Cash_Ratio'] = safe_divide(df['Cash'], df['Current_Liabilities'])\n",
        "    liquidity_count += 1\n",
        "\n",
        "print(f\"    Computed {liquidity_count} liquidity ratios\")\n",
        "computed_count += liquidity_count\n",
        "\n",
        "# ====================\n",
        "# 3. LEVERAGE RATIOS\n",
        "# ====================\n",
        "print(\"3. Leverage/Solvency Ratios\")\n",
        "leverage_count = 0\n",
        "\n",
        "if 'Total_Liabilities' in df.columns and 'Shareholders_Equity' in df.columns:\n",
        "    ratios['Debt_to_Equity'] = safe_divide(df['Total_Liabilities'], df['Shareholders_Equity'])\n",
        "    leverage_count += 1\n",
        "\n",
        "if 'Total_Liabilities' in df.columns and 'Total_Assets' in df.columns:\n",
        "    ratios['Debt_to_Assets'] = safe_divide(df['Total_Liabilities'], df['Total_Assets'])\n",
        "    leverage_count += 1\n",
        "\n",
        "if 'Shareholders_Equity' in df.columns and 'Total_Assets' in df.columns:\n",
        "    ratios['Equity_Ratio'] = safe_divide(df['Shareholders_Equity'], df['Total_Assets'])\n",
        "    leverage_count += 1\n",
        "\n",
        "if 'EBIT' in df.columns and 'Interest_Expense' in df.columns:\n",
        "    ratios['Interest_Coverage'] = safe_divide(df['EBIT'], df['Interest_Expense'])\n",
        "    leverage_count += 1\n",
        "\n",
        "print(f\"    Computed {leverage_count} leverage ratios\")\n",
        "computed_count += leverage_count\n",
        "\n",
        "# ====================\n",
        "# 4. EFFICIENCY RATIOS\n",
        "# ====================\n",
        "print(\"4. Efficiency/Activity Ratios\")\n",
        "efficiency_count = 0\n",
        "\n",
        "if 'Revenue' in df.columns and 'Total_Assets' in df.columns:\n",
        "    ratios['Asset_Turnover'] = safe_divide(df['Revenue'], df['Total_Assets'])\n",
        "    efficiency_count += 1\n",
        "\n",
        "if 'COGS' in df.columns and 'Inventory' in df.columns:\n",
        "    ratios['Inventory_Turnover'] = safe_divide(df['COGS'], df['Inventory'])\n",
        "    efficiency_count += 1\n",
        "\n",
        "if 'Revenue' in df.columns and 'Accounts_Receivable' in df.columns:\n",
        "    ratios['Receivables_Turnover'] = safe_divide(df['Revenue'], df['Accounts_Receivable'])\n",
        "    efficiency_count += 1\n",
        "\n",
        "print(f\"    Computed {efficiency_count} efficiency ratios\")\n",
        "computed_count += efficiency_count\n",
        "\n",
        "# ====================\n",
        "# 5. VALUATION RATIOS\n",
        "# ====================\n",
        "print(\"5. Valuation Ratios\")\n",
        "valuation_count = 0\n",
        "\n",
        "if 'Stock_Price' in df.columns and 'EPS' in df.columns:\n",
        "    ratios['PE_Ratio'] = safe_divide(df['Stock_Price'], df['EPS'])\n",
        "    valuation_count += 1\n",
        "\n",
        "if 'Market_Cap' in df.columns and 'Shareholders_Equity' in df.columns:\n",
        "    ratios['PB_Ratio'] = safe_divide(df['Market_Cap'], df['Shareholders_Equity'])\n",
        "    valuation_count += 1\n",
        "\n",
        "if 'Dividend_Per_Share' in df.columns and 'Stock_Price' in df.columns:\n",
        "    ratios['Dividend_Yield'] = safe_divide(df['Dividend_Per_Share'], df['Stock_Price']) * 100\n",
        "    valuation_count += 1\n",
        "\n",
        "if 'EPS' in df.columns and 'Stock_Price' in df.columns:\n",
        "    ratios['Earnings_Yield'] = safe_divide(df['EPS'], df['Stock_Price']) * 100\n",
        "    valuation_count += 1\n",
        "\n",
        "print(f\"    Computed {valuation_count} valuation ratios\")\n",
        "computed_count += valuation_count\n",
        "\n",
        "# ====================\n",
        "# 6. CASH FLOW RATIOS\n",
        "# ====================\n",
        "print(\"6. Cash Flow Ratios\")\n",
        "cashflow_count = 0\n",
        "\n",
        "if 'Operating_Cash_Flow' in df.columns and 'Current_Liabilities' in df.columns:\n",
        "    ratios['OCF_Ratio'] = safe_divide(df['Operating_Cash_Flow'], df['Current_Liabilities'])\n",
        "    cashflow_count += 1\n",
        "\n",
        "if 'Free_Cash_Flow' in df.columns and 'Shareholders_Equity' in df.columns:\n",
        "    ratios['FCF_to_Equity'] = safe_divide(df['Free_Cash_Flow'], df['Shareholders_Equity'])\n",
        "    cashflow_count += 1\n",
        "\n",
        "if 'Operating_Cash_Flow' in df.columns and 'Total_Liabilities' in df.columns:\n",
        "    ratios['CF_to_Debt'] = safe_divide(df['Operating_Cash_Flow'], df['Total_Liabilities'])\n",
        "    cashflow_count += 1\n",
        "\n",
        "print(f\"    Computed {cashflow_count} cash flow ratios\")\n",
        "computed_count += cashflow_count\n",
        "\n",
        "# ====================\n",
        "# 7. GROWTH METRICS\n",
        "# ====================\n",
        "print(\"7. Growth Metrics (YoY)\")\n",
        "growth_count = 0\n",
        "\n",
        "if 'Company' in df.columns and 'Year' in df.columns:\n",
        "    df = df.sort_values(['Company', 'Year'])\n",
        "    \n",
        "    if 'Revenue' in df.columns:\n",
        "        ratios['Revenue_Growth'] = df.groupby('Company')['Revenue'].pct_change() * 100\n",
        "        growth_count += 1\n",
        "    \n",
        "    if 'Net_Income' in df.columns:\n",
        "        ratios['Earnings_Growth'] = df.groupby('Company')['Net_Income'].pct_change() * 100\n",
        "        growth_count += 1\n",
        "    \n",
        "    if 'Total_Assets' in df.columns:\n",
        "        ratios['Asset_Growth'] = df.groupby('Company')['Total_Assets'].pct_change() * 100\n",
        "        growth_count += 1\n",
        "\n",
        "print(f\"    Computed {growth_count} growth metrics\")\n",
        "computed_count += growth_count\n",
        "\n",
        "# Combine ratios with original data\n",
        "if ratios:\n",
        "    ratios_df = pd.DataFrame(ratios)\n",
        "    df_with_ratios = pd.concat([df, ratios_df], axis=1)\n",
        "    \n",
        "    print(f\"\\n Total ratios computed: {computed_count}\")\n",
        "    print(f\" Dataset shape after adding ratios: {df_with_ratios.shape}\")\n",
        "else:\n",
        "    print(\"\\n WARNING: No ratios could be computed. Check if required columns exist.\")\n",
        "    df_with_ratios = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Handle Infinite and Extreme Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace infinite values with NaN\n",
        "numeric_df = df_with_ratios.select_dtypes(include=[np.number])\n",
        "inf_count_before = np.isinf(numeric_df).sum().sum()\n",
        "df_with_ratios = df_with_ratios.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "if inf_count_before > 0:\n",
        "    print(f\" Replaced {inf_count_before:,} infinite values with NaN\")\n",
        "\n",
        "# Cap extreme values at 1st and 99th percentile for ratios\n",
        "ratio_columns = list(ratios.keys())\n",
        "capped_count = 0\n",
        "\n",
        "if ratio_columns:\n",
        "    for col in ratio_columns:\n",
        "        if df_with_ratios[col].dtype in ['float64', 'int64']:\n",
        "            p1 = df_with_ratios[col].quantile(0.01)\n",
        "            p99 = df_with_ratios[col].quantile(0.99)\n",
        "            outliers = ((df_with_ratios[col] < p1) | (df_with_ratios[col] > p99)).sum()\n",
        "            \n",
        "            if outliers > 0:\n",
        "                df_with_ratios[col] = df_with_ratios[col].clip(lower=p1, upper=p99)\n",
        "                capped_count += outliers\n",
        "    \n",
        "    if capped_count > 0:\n",
        "        print(f\" Capped {capped_count:,} extreme values at 1st/99th percentile\")\n",
        "    \n",
        "    # Impute remaining NaN values in ratios\n",
        "    nan_count = df_with_ratios[ratio_columns].isnull().sum().sum()\n",
        "    if nan_count > 0:\n",
        "        imputer_ratios = SimpleImputer(strategy='median')\n",
        "        df_with_ratios[ratio_columns] = imputer_ratios.fit_transform(df_with_ratios[ratio_columns])\n",
        "        print(f\" Imputed {nan_count:,} NaN values in ratios using median\")\n",
        "\n",
        "final_missing = df_with_ratios.isnull().sum().sum()\n",
        "print(f\"\\n Final missing values: {final_missing:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Correlation Analysis & Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Analyze Correlation Among Ratios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ratio_columns = list(ratios.keys())\n",
        "\n",
        "if len(ratio_columns) >= 2:\n",
        "    # Calculate correlation matrix for ratios only\n",
        "    ratio_corr = df_with_ratios[ratio_columns].corr()\n",
        "    \n",
        "    # Visualize correlation matrix\n",
        "    plt.figure(figsize=(16, 14))\n",
        "    sns.heatmap(ratio_corr, annot=False, fmt='.2f', cmap='coolwarm', center=0,\n",
        "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title('Correlation Matrix of Financial Ratios', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Find highly correlated ratio pairs (> 0.95)\n",
        "    high_corr_ratios = []\n",
        "    for i in range(len(ratio_corr)):\n",
        "        for j in range(i+1, len(ratio_corr)):\n",
        "            if abs(ratio_corr.iloc[i, j]) > 0.95:\n",
        "                high_corr_ratios.append({\n",
        "                    'Feature_1': ratio_corr.index[i],\n",
        "                    'Feature_2': ratio_corr.columns[j],\n",
        "                    'Correlation': ratio_corr.iloc[i, j]\n",
        "                })\n",
        "    \n",
        "    if high_corr_ratios:\n",
        "        high_corr_df = pd.DataFrame(high_corr_ratios).sort_values('Correlation', key=abs, ascending=False)\n",
        "        print(f\"\\n Found {len(high_corr_ratios)} highly correlated ratio pairs (|r| > 0.95)\")\n",
        "        print(\"\\nTop 10 Most Correlated Pairs:\")\n",
        "        print(high_corr_df.head(10).to_string(index=False))\n",
        "    else:\n",
        "        print(\"\\n No highly correlated ratio pairs found (|r| > 0.95)\")\n",
        "else:\n",
        "    print(\" Not enough ratios for correlation analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Remove Redundant Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build list of features to drop based on high correlation\n",
        "features_to_drop = []\n",
        "\n",
        "if len(ratio_columns) >= 2:\n",
        "    for i in range(len(ratio_corr.columns)):\n",
        "        for j in range(i+1, len(ratio_corr.columns)):\n",
        "            if abs(ratio_corr.iloc[i, j]) > 0.95:\n",
        "                feature_to_drop = ratio_corr.columns[j]\n",
        "                if feature_to_drop not in features_to_drop:\n",
        "                    features_to_drop.append(feature_to_drop)\n",
        "                    print(f\"  Dropping: {feature_to_drop} (corr={ratio_corr.iloc[i, j]:.3f} with {ratio_corr.columns[i]})\")\n",
        "\n",
        "# Create final dataset with selected features\n",
        "id_columns = ['Company', 'Sector', 'Year', 'Quarter']\n",
        "existing_id_cols = [col for col in id_columns if col in df_with_ratios.columns]\n",
        "\n",
        "columns_to_keep = existing_id_cols + \\\n",
        "                  [col for col in ratio_columns if col not in features_to_drop]\n",
        "\n",
        "if 'Investment_Quality' in df_with_ratios.columns:\n",
        "    columns_to_keep.append('Investment_Quality')\n",
        "\n",
        "df_final = df_with_ratios[columns_to_keep].copy()\n",
        "\n",
        "if features_to_drop:\n",
        "    print(f\"\\n Removed {len(features_to_drop)} redundant features\")\n",
        "else:\n",
        "    print(f\"\\n No redundant features to remove\")\n",
        "\n",
        "final_feature_count = len([c for c in df_final.columns if c not in existing_id_cols + ['Investment_Quality']])\n",
        "print(f\" Final dataset shape: {df_final.shape}\")\n",
        "print(f\" Final feature count: {final_feature_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display final feature list\n",
        "exclude_cols = existing_id_cols + ['Investment_Quality']\n",
        "final_features = [col for col in df_final.columns if col not in exclude_cols]\n",
        "\n",
        "print(f\"\\nFinal Feature List ({len(final_features)} features):\")\n",
        "print(\"=\"*60)\n",
        "for i, feature in enumerate(final_features, 1):\n",
        "    print(f\"{i:2d}. {feature}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 6: Model Training & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Prepare Data for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if target variable exists\n",
        "if 'Investment_Quality' not in df_final.columns:\n",
        "    print(\" ERROR: 'Investment_Quality' target variable not found!\")\n",
        "    print(\"   Cannot proceed with model training.\")\n",
        "    print(\"   Please ensure your data has an 'Investment_Quality' column with 'Good' or 'Bad' labels.\")\n",
        "else:\n",
        "    # Separate features and target\n",
        "    exclude_cols = existing_id_cols + ['Investment_Quality']\n",
        "    feature_cols = [col for col in df_final.columns if col not in exclude_cols]\n",
        "    \n",
        "    X = df_final[feature_cols].copy()\n",
        "    y = df_final['Investment_Quality'].copy()\n",
        "    \n",
        "    print(f\" Features: {len(feature_cols)}\")\n",
        "    print(f\" Target classes: {y.unique()}\")\n",
        "    print(f\"\\nClass distribution:\")\n",
        "    print(y.value_counts())\n",
        "    print(f\"\\nClass percentages:\")\n",
        "    print((y.value_counts(normalize=True) * 100).round(2))\n",
        "    \n",
        "    # Encode target variable\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "    print(f\"\\n Target encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Investment_Quality' in df_final.columns and len(feature_cols) > 0:\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "    \n",
        "    print(f\" Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "    print(f\" Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "    print(f\"\\nTraining class distribution: {dict(zip(le.classes_, np.bincount(y_train)))}\")\n",
        "    print(f\"Test class distribution: {dict(zip(le.classes_, np.bincount(y_test)))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Investment_Quality' in df_final.columns and len(feature_cols) > 0:\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    print(f\" Features scaled using StandardScaler\")\n",
        "    print(f\"  Training set mean: {X_train_scaled.mean():.6f}\")\n",
        "    print(f\"  Training set std: {X_train_scaled.std():.6f}\")\n",
        "    print(f\"  Test set mean: {X_test_scaled.mean():.6f}\")\n",
        "    print(f\"  Test set std: {X_test_scaled.std():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Train Multiple Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Investment_Quality' in df_final.columns and len(feature_cols) > 0:\n",
        "    # Initialize models\n",
        "    class ConventionalValueModel:\n",
        "        \"\"\"A simple rule-based model based on standard financial benchmarks\"\"\"\n",
        "        def fit(self, X, y): \n",
        "            pass\n",
        "        \n",
        "        def predict(self, X):\n",
        "            # Simple rule: classify as 'Good' if first feature (after scaling) is positive\n",
        "            # This is a simplified baseline - real conventional methods use unscaled thresholds\n",
        "            return np.where(X[:, 0] > 0, 1, 0)\n",
        "        \n",
        "        def predict_proba(self, X):\n",
        "            p = self.predict(X)\n",
        "            return np.column_stack((1-p, p))\n",
        "    \n",
        "    models = {\n",
        "        'Conventional Rule-Based': ConventionalValueModel(),\n",
        "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "        'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
        "        'SVM (RBF)': SVC(kernel='rbf', random_state=42, probability=True),\n",
        "        'Neural Network': MLPClassifier(hidden_layer_sizes=(64, 32), random_state=42, max_iter=1000)\n",
        "    }\n",
        "    \n",
        "    print(f\" Initialized {len(models)} models (including Conventional Baseline)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Investment_Quality' in df_final.columns and len(feature_cols) > 0:\n",
        "    # Train and evaluate all models\n",
        "    results = []\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING MODELS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n[{name}]\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        # Train\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        print(\"Training completed\")\n",
        "        \n",
        "        # Cross-validation\n",
        "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
        "        \n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "        \n",
        "        # Metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else np.nan\n",
        "        \n",
        "        # Store results\n",
        "        results.append({\n",
        "            'Model': name,\n",
        "            'Accuracy': accuracy,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1-Score': f1,\n",
        "            'ROC-AUC': roc_auc,\n",
        "            'CV Mean': cv_scores.mean(),\n",
        "            'CV Std': cv_scores.std()\n",
        "        })\n",
        "        \n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "        print(f\"  Recall: {recall:.4f}\")\n",
        "        print(f\"  F1-Score: {f1:.4f}\")\n",
        "        if not np.isnan(roc_auc):\n",
        "            print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
        "        print(f\"  CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"All models trained successfully!\")\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.5 Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Investment_Quality' in df_final.columns and len(feature_cols) > 0:\n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df.sort_values('Accuracy', ascending=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"MODEL PERFORMANCE SUMMARY (sorted by Accuracy)\")\n",
        "    print(\"=\"*100)\n",
        "    print(results_df.to_string(index=False))\n",
        "    print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Investment_Quality' in df_final.columns and len(feature_cols) > 0:\n",
        "    # Visualize model comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Accuracy comparison\n",
        "    results_df_sorted = results_df.sort_values('Accuracy')\n",
        "    axes[0].barh(results_df_sorted['Model'], results_df_sorted['Accuracy'], color='steelblue')\n",
        "    axes[0].set_xlabel('Accuracy', fontsize=12)\n",
        "    axes[0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlim([0, 1.05])\n",
        "    axes[0].grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, v in enumerate(results_df_sorted['Accuracy']):\n",
        "        axes[0].text(v + 0.01, i, f'{v:.4f}', va='center')\n",
        "    \n",
        "    # Multiple metrics comparison\n",
        "    metrics_df = results_df_sorted[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score']].set_index('Model')\n",
        "    metrics_df.plot(kind='barh', ax=axes[1])\n",
        "    axes[1].set_xlabel('Score', fontsize=12)\n",
        "    axes[1].set_title('Multiple Metrics Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlim([0, 1.05])\n",
        "    axes[1].legend(loc='lower right', fontsize=10)\n",
        "    axes[1].grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.5.5 Comparison to Conventional Methods & Financial Implications\n",
        "\n",
        "A key part of our analysis is comparing Machine Learning to conventional \"Rule-Based\" investing (e.g., Benjamin Graham style value investing).\n",
        "\n",
        "**Key Financial Insights:**\n",
        "1. **Conventional vs. ML**: Conventional rules often miss subtle non-linear relationships between ratios that ML can capture. While conventional methods are transparent, ML models often achieve higher accuracy by identifying complex risk patterns.\n",
        "2. **The Cost of Errors (False Positives)**: In long-term equity investing, a **False Positive** (classifying a failing company as 'Good') is far more expensive than a **False Negative** (missing a good company). A False Positive leads to permanent capital loss, whereas a False Negative only leads to opportunity cost.\n",
        "3. **Interpretation**: We focus on models that provide high **Precision** for the 'Good' class, as this minimizes the risk of buying into a bad investment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.6 Best Model Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Investment_Quality' in df_final.columns and len(feature_cols) > 0:\n",
        "    # Get best model\n",
        "    best_model_name = results_df.iloc[0]['Model']\n",
        "    best_model = models[best_model_name]\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BEST MODEL ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\n Best Model: {best_model_name}\")\n",
        "    print(f\"   Accuracy: {results_df.iloc[0]['Accuracy']:.4f}\")\n",
        "    print(f\"   F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
        "    print(f\"   ROC-AUC: {results_df.iloc[0]['ROC-AUC']:.4f}\")\n",
        "    \n",
        "    # Predictions from best model\n",
        "    y_pred_best = best_model.predict(X_test_scaled)\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred_best)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "    \n",
        "    # Visualize confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=le.classes_, yticklabels=le.classes_,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Actual', fontsize=12)\n",
        "    plt.xlabel('Predicted', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(\"=\"*80)\n",
        "    print(classification_report(y_test, y_pred_best, target_names=le.classes_))\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.7 Feature Importance (if available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Investment_Quality' in df_final.columns and len(feature_cols) > 0:\n",
        "    # Feature importance for tree-based models\n",
        "    if hasattr(best_model, 'feature_importances_'):\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'Feature': feature_cols,\n",
        "            'Importance': best_model.feature_importances_\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"\\nTop 10 Most Important Features:\")\n",
        "        print(feature_importance.head(10).to_string(index=False))\n",
        "        \n",
        "        # Visualize\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        top_features = feature_importance.head(15)\n",
        "        plt.barh(range(len(top_features)), top_features['Importance'], color='steelblue')\n",
        "        plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "        plt.xlabel('Importance', fontsize=12)\n",
        "        plt.title(f'Top 15 Feature Importances - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    elif hasattr(best_model, 'coef_'):\n",
        "        feature_coefs = pd.DataFrame({\n",
        "            'Feature': feature_cols,\n",
        "            'Coefficient': best_model.coef_[0]\n",
        "        }).sort_values('Coefficient', key=abs, ascending=False)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"FEATURE COEFFICIENT ANALYSIS\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"\\nTop 10 Features by Coefficient Magnitude:\")\n",
        "        print(feature_coefs.head(10).to_string(index=False))\n",
        "        \n",
        "        # Visualize\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        top_features = feature_coefs.head(15)\n",
        "        colors = ['green' if x > 0 else 'red' for x in top_features['Coefficient']]\n",
        "        plt.barh(range(len(top_features)), top_features['Coefficient'], color=colors, alpha=0.7)\n",
        "        plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "        plt.xlabel('Coefficient', fontsize=12)\n",
        "        plt.title(f'Top 15 Feature Coefficients - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"\\n Feature importance/coefficients not available for this model type\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary & Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\" \"*30 + \"PROJECT SUMMARY\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "print(f\"\\n Dataset:\")\n",
        "print(f\"   Companies: {df_raw['Company'].nunique() if 'Company' in df_raw.columns else 'N/A'}\")\n",
        "print(f\"   Time Period: {df_raw['Year'].min() if 'Year' in df_raw.columns else 'N/A'} - {df_raw['Year'].max() if 'Year' in df_raw.columns else 'N/A'}\")\n",
        "print(f\"   Total Records: {len(df_raw):,}\")\n",
        "if 'Sector' in df_raw.columns:\n",
        "    print(f\"   Sectors: {df_raw['Sector'].nunique()}\")\n",
        "\n",
        "if len(feature_cols) > 0:\n",
        "    print(f\"   Final Features: {len(feature_cols)}\")\n",
        "\n",
        "print(f\"\\n Preprocessing:\")\n",
        "if 'missing_before' in locals():\n",
        "    print(f\"   Missing values imputed: {missing_before:,}\")\n",
        "if ratios:\n",
        "    print(f\"   Financial ratios computed: {len(ratios)}\")\n",
        "if features_to_drop:\n",
        "    print(f\"   Redundant features removed: {len(features_to_drop)}\")\n",
        "\n",
        "if 'Investment_Quality' in df_final.columns and len(feature_cols) > 0:\n",
        "    print(f\"\\n Models:\")\n",
        "    print(f\"   Total models evaluated: {len(models)}\")\n",
        "    print(f\"   Best model: {best_model_name}\")\n",
        "    print(f\"   Best accuracy: {results_df.iloc[0]['Accuracy']:.4f}\")\n",
        "    print(f\"   Best F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
        "    \n",
        "    conv_result = results_df[results_df['Model'] == 'Conventional Rule-Based']\n",
        "    if not conv_result.empty:\n",
        "        conv_acc = conv_result['Accuracy'].values[0]\n",
        "        print(f\"\\n Conventional vs Machine Learning:\")\n",
        "        print(f\"   Conventional Rule-Based Accuracy: {conv_acc:.4f}\")\n",
        "        print(f\"   ML Performance Lift: {((results_df.iloc[0]['Accuracy'] / conv_acc) - 1) * 100:.1f}%\")\n",
        "    \n",
        "    print(f\"\\n Financial Implications:\")\n",
        "    print(f\"   Our best model focuses on minimizing False Positives (Permanent Capital Loss).\")\n",
        "    print(f\"   By using non-linear combinations of {len(feature_cols)} ratios, we outperform standard benchmarks.\")\n",
        "    \n",
        "    print(f\"\\n Top 3 Models:\")\n",
        "    for i, row in results_df.head(3).iterrows():\n",
        "        print(f\"   {row['Model']:30s}: Accuracy = {row['Accuracy']:.4f}, F1 = {row['F1-Score']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\" \"*25 + \" ANALYSIS COMPLETE\")\n",
        "print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ðŸš€ How to Use This for Predicting New Companies\n",
        "\n",
        "To predict whether a new/real company is a 'Good' or 'Bad' investment:\n",
        "\n",
        "### Step 1: Prepare Your Data\n",
        "Create a dictionary or DataFrame with the company's financial data matching the format of your training data.\n",
        "\n",
        "### Step 2: Calculate Ratios\n",
        "Apply the same ratio formulas from **Part 4** to your new data.\n",
        "\n",
        "### Step 3: Scale & Predict\n",
        "Use the `scaler` and `best_model` trained in this notebook.\n",
        "\n",
        "```python\n",
        "# Example Usage:\n",
        "# new_company_data = pd.DataFrame([{\n",
        "#     'Revenue': 1000,\n",
        "#     'Net_Income': 100,\n",
        "#     'Total_Assets': 5000,\n",
        "#     # ... all other required columns\n",
        "# }])\n",
        "\n",
        "# # Calculate ratios using the same logic from Part 4\n",
        "# new_ratios = calculate_ratios(new_company_data)\n",
        "\n",
        "# # Keep only the features used in training\n",
        "# new_features = new_ratios[feature_cols]\n",
        "\n",
        "# # Scale using the fitted scaler\n",
        "# new_scaled = scaler.transform(new_features)\n",
        "\n",
        "# # Make prediction\n",
        "# prediction = best_model.predict(new_scaled)\n",
        "# prediction_proba = best_model.predict_proba(new_scaled)\n",
        "\n",
        "# # Display result\n",
        "# result = le.inverse_transform(prediction)[0]\n",
        "# confidence = prediction_proba[0][prediction[0]]\n",
        "# print(f\"Prediction: {result} (Confidence: {confidence:.2%})\")\n",
        "```\n",
        "\n",
        "### Important Notes:\n",
        "- Ensure new data has the same columns as training data\n",
        "- Handle missing values the same way (median imputation)\n",
        "- Apply the same ratio calculations\n",
        "- Use the same feature selection (drop redundant features)\n",
        "- Scale using the fitted scaler (don't refit on new data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir = 'analysis_outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"Exporting results...\\n\")\n",
        "\n",
        "# Save processed data with ratios\n",
        "output_file = os.path.join(output_dir, 'processed_financial_data_with_ratios.csv')\n",
        "df_final.to_csv(output_file, index=False)\n",
        "print(f\" Saved: {output_file}\")\n",
        "\n",
        "# Save model results\n",
        "if 'results_df' in locals():\n",
        "    output_file = os.path.join(output_dir, 'model_comparison_results.csv')\n",
        "    results_df.to_csv(output_file, index=False)\n",
        "    print(f\" Saved: {output_file}\")\n",
        "\n",
        "# Save feature importance\n",
        "if 'feature_importance' in locals():\n",
        "    output_file = os.path.join(output_dir, 'feature_importance.csv')\n",
        "    feature_importance.to_csv(output_file, index=False)\n",
        "    print(f\" Saved: {output_file}\")\n",
        "elif 'feature_coefs' in locals():\n",
        "    output_file = os.path.join(output_dir, 'feature_coefficients.csv')\n",
        "    feature_coefs.to_csv(output_file, index=False)\n",
        "    print(f\" Saved: {output_file}\")\n",
        "\n",
        "# Save list of features used\n",
        "if 'feature_cols' in locals():\n",
        "    output_file = os.path.join(output_dir, 'features_used.txt')\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(\"Features used in final model:\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        for i, feat in enumerate(feature_cols, 1):\n",
        "            f.write(f\"{i:2d}. {feat}\\n\")\n",
        "    print(f\" Saved: {output_file}\")\n",
        "\n",
        "print(f\"\\n All results exported to '{output_dir}/' directory!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Next Steps\n",
        "\n",
        "### To improve this analysis:\n",
        "\n",
        "1. **Hyperparameter Tuning**: Use GridSearchCV to optimize model parameters\n",
        "2. **Feature Engineering**: Create additional derived ratios or interaction terms\n",
        "3. **Ensemble Methods**: Combine multiple models for better predictions\n",
        "4. **Time-Series Analysis**: Account for temporal patterns if you have multi-year data\n",
        "5. **Industry-Specific Models**: Build separate models for different sectors\n",
        "6. **Explainability**: Use SHAP values for better model interpretability\n",
        "7. **Real-Time Data**: Integrate with financial APIs for live predictions\n",
        "8. **Backtesting**: Validate predictions against actual stock performance\n",
        "\n",
        "### Remember:\n",
        "- This is a classification model, not investment advice\n",
        "- Always validate predictions with fundamental analysis\n",
        "- Markets are complex - no model is perfect\n",
        "- Past performance doesn't guarantee future results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
